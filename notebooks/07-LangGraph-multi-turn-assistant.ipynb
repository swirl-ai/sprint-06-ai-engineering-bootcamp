{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Prefetch, Filter, FieldCondition, MatchText, FusionQuery\n",
    "\n",
    "from langsmith import traceable, get_current_run_tree\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "from langchain_core.messages import AIMessage, ToolMessage\n",
    "\n",
    "from jinja2 import Template\n",
    "from typing import Literal, Dict, Any, Annotated, List, Optional\n",
    "from IPython.display import Image, display\n",
    "from operator import add\n",
    "from openai import OpenAI\n",
    "\n",
    "import openai\n",
    "\n",
    "import random\n",
    "import ast\n",
    "import inspect\n",
    "import instructor\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@traceable(\n",
    "    name=\"embed_query\",\n",
    "    run_type=\"embedding\",\n",
    "    metadata={\"ls_provider\": \"openai\", \"ls_model_name\": \"text-embedding-3-small\"}\n",
    ")\n",
    "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
    "    response = openai.embeddings.create(\n",
    "        input=[text],\n",
    "        model=model,\n",
    "    )\n",
    "\n",
    "    current_run = get_current_run_tree()\n",
    "    if current_run:\n",
    "        current_run.metadata[\"usage_metadata\"] = {\n",
    "            \"input_tokens\": response.usage.prompt_tokens,\n",
    "            \"total_tokens\": response.usage.total_tokens,\n",
    "        }\n",
    "\n",
    "    return response.data[0].embedding\n",
    "\n",
    "\n",
    "@traceable(\n",
    "    name=\"retrieve_top_n\",\n",
    "    run_type=\"retriever\"\n",
    ")\n",
    "def retrieve_context(query, top_k=5):\n",
    "    query_embedding = get_embedding(query)\n",
    "\n",
    "    qdrant_client = QdrantClient(url=\"http://localhost:6333\")\n",
    "\n",
    "    results = qdrant_client.query_points(\n",
    "        collection_name=\"Amazon-items-collection-01-hybrid\",\n",
    "        prefetch=[\n",
    "            Prefetch(\n",
    "                query=query_embedding,\n",
    "                limit=20\n",
    "            ),\n",
    "            Prefetch(\n",
    "                filter=Filter(\n",
    "                    must=[\n",
    "                        FieldCondition(\n",
    "                            key=\"text\",\n",
    "                            match=MatchText(text=query)\n",
    "                        )\n",
    "                    ]\n",
    "                ),\n",
    "                limit=20\n",
    "            )\n",
    "        ],\n",
    "        query=FusionQuery(fusion=\"rrf\"),\n",
    "        limit=top_k\n",
    "    )\n",
    "\n",
    "    retrieved_context_ids = []\n",
    "    retrieved_context = []\n",
    "    similarity_scores = []\n",
    "\n",
    "    for result in results.points:\n",
    "        retrieved_context_ids.append(result.id)\n",
    "        retrieved_context.append(result.payload['text'])\n",
    "        similarity_scores.append(result.score)\n",
    "\n",
    "    return {\n",
    "        \"retrieved_context_ids\": retrieved_context_ids,\n",
    "        \"retrieved_context\": retrieved_context,\n",
    "        \"similarity_scores\": similarity_scores\n",
    "    }\n",
    "\n",
    "\n",
    "@traceable(\n",
    "    name=\"format_retrieved_context\",\n",
    "    run_type=\"prompt\"\n",
    ")\n",
    "def process_context(context):\n",
    "\n",
    "    formatted_context = \"\"\n",
    "\n",
    "    for id, chunk in zip(context[\"retrieved_context_ids\"], context[\"retrieved_context\"]):\n",
    "        formatted_context += f\"- {id}: {chunk}\\n\"\n",
    "\n",
    "    return formatted_context\n",
    "\n",
    "\n",
    "def get_formatted_context(query: str, top_k: int = 5) -> str:\n",
    "\n",
    "    \"\"\"Get the top k context, each representing an inventory item for a given query.\n",
    "    \n",
    "    Args:\n",
    "        query: The query to get the top k context for\n",
    "        top_k: The number of context chunks to retrieve, works best with 5 or more\n",
    "    \n",
    "    Returns:\n",
    "        A string of the top k context chunks with IDs prepending each chunk, each representing an inventory item for a given query.\n",
    "    \"\"\"\n",
    "\n",
    "    context = retrieve_context(query, top_k)\n",
    "    formatted_context = process_context(context)\n",
    "\n",
    "    return formatted_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State and Pydantic Models for structured outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToolCall(BaseModel):\n",
    "    name: str\n",
    "    arguments: dict\n",
    "\n",
    "class RAGUsedContext(BaseModel):\n",
    "    id: int\n",
    "    description: str\n",
    "\n",
    "class AgentResponse(BaseModel):\n",
    "    answer: str\n",
    "    tool_calls: List[ToolCall] = Field(default_factory=list)\n",
    "    final_answer: bool = Field(default=False)\n",
    "    retrieved_context_ids: List[RAGUsedContext]\n",
    "\n",
    "class State(BaseModel):\n",
    "    messages: Annotated[List[Any], add] = []\n",
    "    answer: str = \"\"\n",
    "    iteration: int = Field(default=0)\n",
    "    final_answer: bool = Field(default=False)\n",
    "    available_tools: List[Dict[str, Any]] = []\n",
    "    tool_calls: Optional[List[ToolCall]] = Field(default_factory=list)\n",
    "    retrieved_context_ids: Annotated[List[RAGUsedContext], add] = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@traceable(\n",
    "    name=\"agent_node\",\n",
    "    run_type=\"llm\",\n",
    "    metadata={\"ls_provider\": \"openai\", \"ls_model_name\": \"gpt-4.1-mini\"}\n",
    ")\n",
    "def agent_node(state: State) -> dict:\n",
    "\n",
    "   prompt_template =  \"\"\"You are a shopping assistant that can answer questions about the products in stock.\n",
    "\n",
    "You will be given a question and a list of tools you can use to answer that question.\n",
    "\n",
    "<Available tools>\n",
    "{{ available_tools | tojson }}\n",
    "</Available tools>\n",
    "\n",
    "After the tools are used you will get the outputs from the tools.\n",
    "\n",
    "When you need to use a tool, format your response as:\n",
    "\n",
    "<tool_call>\n",
    "{\"name\": \"tool_name\", \"arguments\": {...}}\n",
    "</tool_call>\n",
    "\n",
    "Use names specificly provided in the available tools. Don't add any additional text to the names.\n",
    "\n",
    "You should tend to use tools when additional information is needed to answer the question.\n",
    "\n",
    "If you set final_answer to True, you should not use any tools.\n",
    "\n",
    "Instructions:\n",
    "- You need to answer the question based on the retrieved context using the available tools only.\n",
    "- Never use word context and refer to it as the available products.\n",
    "- You should only answer questions about the products in stock. If the question is not about the products in stock, you should ask for clarification.\n",
    "- As a final output you need to provide:\n",
    "\n",
    "* The answer to the question based on the retrieved context.\n",
    "* The list of the indexes from the chunks returned from all tool calls that were used to answer the question. If more than one chunk was used to compile the answer from a single tool call, be sure to return all of them.\n",
    "* Short description of the item based on the retrieved context.\n",
    "\n",
    "- The answer to the question should contain detailed information about the product and returned with detailed specification in bullet points.\n",
    "- The short description should have the name of the item.\n",
    "- If the user's request requires using a tool, set tool_calls with the appropriate function name and arguments.\n",
    "- If you have all the information needed to provide a complete answer, set final_answer to True.\n",
    "\"\"\"\n",
    "\n",
    "   template = Template(prompt_template)\n",
    "   \n",
    "   prompt = template.render(\n",
    "      available_tools=state.available_tools\n",
    "   )\n",
    "\n",
    "   messages = state.messages\n",
    "\n",
    "   conversation = []\n",
    "\n",
    "   for msg in messages:\n",
    "      conversation.append(lc_messages_to_regular_messages(msg))\n",
    "\n",
    "   client = instructor.from_openai(OpenAI())\n",
    "\n",
    "   response, raw_response = client.chat.completions.create_with_completion(\n",
    "        model=\"gpt-4.1-mini\",\n",
    "        response_model=AgentResponse,\n",
    "        messages=[{\"role\": \"system\", \"content\": prompt}, *conversation],\n",
    "        temperature=0.5,\n",
    "   )\n",
    "\n",
    "   if response.tool_calls:\n",
    "      tool_calls = []\n",
    "      for i, tc in enumerate(response.tool_calls):\n",
    "         tool_calls.append({\n",
    "               \"id\": f\"call_{i}\",\n",
    "               \"name\": tc.name,\n",
    "               \"args\": tc.arguments\n",
    "         })\n",
    "\n",
    "      ai_message = AIMessage(\n",
    "         content=response.answer,\n",
    "         tool_calls=tool_calls\n",
    "         )\n",
    "   else:\n",
    "      ai_message = AIMessage(\n",
    "         content=response.answer,\n",
    "      )\n",
    "\n",
    "   return {\n",
    "      \"messages\": [ai_message],\n",
    "      \"tool_calls\": response.tool_calls,\n",
    "      \"iteration\": state.iteration + 1,\n",
    "      \"answer\": response.answer,\n",
    "      \"final_answer\": response.final_answer,\n",
    "      \"retrieved_context_ids\": response.retrieved_context_ids\n",
    "   }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Router Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tool_router(state: State) -> str:\n",
    "    \"\"\"Decide whether to continue or end\"\"\"\n",
    "    \n",
    "    if state.final_answer:\n",
    "        return \"end\"\n",
    "    elif state.iteration > 2:\n",
    "        return \"end\"\n",
    "    elif len(state.tool_calls) > 0:\n",
    "        return \"tools\"\n",
    "    else:\n",
    "        return \"end\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(State)\n",
    "\n",
    "tools = [get_formatted_context]\n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "tool_descriptions = get_tool_descriptions_from_node(tool_node)\n",
    "\n",
    "workflow.add_node(\"agent_node\", agent_node)\n",
    "workflow.add_node(\"tool_node\", tool_node)\n",
    "\n",
    "workflow.add_edge(START, \"agent_node\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent_node\",\n",
    "    tool_router,\n",
    "    {\n",
    "        \"tools\": \"tool_node\",\n",
    "        \"end\": END\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"tool_node\", \"agent_node\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persisten state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.postgres import PostgresSaver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the database once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with PostgresSaver.from_conn_string(\"postgresql://langgraph_user:langgraph_password@localhost:5433/langgraph_db\") as checkpointer:\n",
    "\n",
    "    checkpointer.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiturn conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Can I get earphones for myself, a laptop bag for my wife and something cool for my kids?\"}],\n",
    "    \"available_tools\": tool_descriptions\n",
    "}\n",
    "config = {\"configurable\": {\"thread_id\": \"test00045\"}}\n",
    "\n",
    "with PostgresSaver.from_conn_string(\"postgresql://langgraph_user:langgraph_password@localhost:5433/langgraph_db\") as checkpointer:\n",
    "\n",
    "    graph = workflow.compile(checkpointer=checkpointer)\n",
    "\n",
    "    graph.invoke(state, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"And what of these are waterproof and outdoors friendly?\"}],\n",
    "    \"iteration\": 0,\n",
    "    \"available_tools\": tool_descriptions\n",
    "}\n",
    "config = {\"configurable\": {\"thread_id\": \"test00045\"}}\n",
    "\n",
    "with PostgresSaver.from_conn_string(\"postgresql://langgraph_user:langgraph_password@localhost:5433/langgraph_db\") as checkpointer:\n",
    "\n",
    "    graph = workflow.compile(checkpointer=checkpointer)\n",
    "\n",
    "    graph.invoke(state, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Can you get me a speaker, I would love to get at least 6 suggestions\"}],\n",
    "    \"iteration\": 0,\n",
    "    \"available_tools\": tool_descriptions\n",
    "}\n",
    "config = {\"configurable\": {\"thread_id\": \"test0002\"}}\n",
    "\n",
    "with PostgresSaver.from_conn_string(\"postgresql://langgraph_user:langgraph_password@localhost:5433/langgraph_db\") as checkpointer:\n",
    "\n",
    "    graph = workflow.compile(checkpointer=checkpointer)\n",
    "\n",
    "    graph.invoke(state, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_function_definition(function_def: str) -> Dict[str, Any]:\n",
    "    \"\"\"Parse a function definition string to extract metadata including type hints.\"\"\"\n",
    "    result = {\n",
    "        \"name\": \"\",\n",
    "        \"description\": \"\",\n",
    "        \"parameters\": {\"type\": \"object\", \"properties\": {}},\n",
    "        \"required\": [],\n",
    "        \"returns\": {\"type\": \"string\", \"description\": \"\"}\n",
    "    }\n",
    "    \n",
    "    # Parse the function using AST\n",
    "    tree = ast.parse(function_def.strip())\n",
    "    if not tree.body or not isinstance(tree.body[0], ast.FunctionDef):\n",
    "        return result\n",
    "        \n",
    "    func = tree.body[0]\n",
    "    result[\"name\"] = func.name\n",
    "    \n",
    "    # Extract docstring\n",
    "    docstring = ast.get_docstring(func) or \"\"\n",
    "    if docstring:\n",
    "        # Extract description (first line/paragraph)\n",
    "        desc_end = docstring.find('\\n\\n') if '\\n\\n' in docstring else docstring.find('\\nArgs:')\n",
    "        desc_end = desc_end if desc_end > 0 else docstring.find('\\nParameters:')\n",
    "        result[\"description\"] = docstring[:desc_end].strip() if desc_end > 0 else docstring.strip()\n",
    "        \n",
    "        # Parse parameter descriptions\n",
    "        param_descs = parse_docstring_params(docstring)\n",
    "        \n",
    "        # Extract return description\n",
    "        if \"Returns:\" in docstring:\n",
    "            result[\"returns\"][\"description\"] = docstring.split(\"Returns:\")[1].strip().split('\\n')[0]\n",
    "    \n",
    "    # Extract parameters with type hints\n",
    "    args = func.args\n",
    "    defaults = args.defaults\n",
    "    num_args = len(args.args)\n",
    "    num_defaults = len(defaults)\n",
    "    \n",
    "    for i, arg in enumerate(args.args):\n",
    "        if arg.arg == 'self':\n",
    "            continue\n",
    "            \n",
    "        param_info = {\n",
    "            \"type\": get_type_from_annotation(arg.annotation) if arg.annotation else \"string\",\n",
    "            \"description\": param_descs.get(arg.arg, \"\")\n",
    "        }\n",
    "        \n",
    "        # Check for default value\n",
    "        default_idx = i - (num_args - num_defaults)\n",
    "        if default_idx >= 0:\n",
    "            param_info[\"default\"] = ast.literal_eval(ast.unparse(defaults[default_idx]))\n",
    "        else:\n",
    "            result[\"required\"].append(arg.arg)\n",
    "        \n",
    "        result[\"parameters\"][\"properties\"][arg.arg] = param_info\n",
    "    \n",
    "    # Extract return type\n",
    "    if func.returns:\n",
    "        result[\"returns\"][\"type\"] = get_type_from_annotation(func.returns)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def get_type_from_annotation(annotation) -> str:\n",
    "    \"\"\"Convert AST annotation to type string.\"\"\"\n",
    "    if not annotation:\n",
    "        return \"string\"\n",
    "    \n",
    "    type_map = {\n",
    "        'str': 'string',\n",
    "        'int': 'integer', \n",
    "        'float': 'number',\n",
    "        'bool': 'boolean',\n",
    "        'list': 'array',\n",
    "        'dict': 'object',\n",
    "        'List': 'array',\n",
    "        'Dict': 'object'\n",
    "    }\n",
    "    \n",
    "    if isinstance(annotation, ast.Name):\n",
    "        return type_map.get(annotation.id, annotation.id)\n",
    "    elif isinstance(annotation, ast.Subscript) and isinstance(annotation.value, ast.Name):\n",
    "        base_type = annotation.value.id\n",
    "        return type_map.get(base_type, base_type.lower())\n",
    "    \n",
    "    return \"string\"\n",
    "\n",
    "\n",
    "def parse_docstring_params(docstring: str) -> Dict[str, str]:\n",
    "    \"\"\"Extract parameter descriptions from docstring (handles both Args: and Parameters: formats).\"\"\"\n",
    "    params = {}\n",
    "    lines = docstring.split('\\n')\n",
    "    in_params = False\n",
    "    current_param = None\n",
    "    \n",
    "    for line in lines:\n",
    "        stripped = line.strip()\n",
    "        \n",
    "        # Check for parameter section start\n",
    "        if stripped in ['Args:', 'Arguments:', 'Parameters:', 'Params:']:\n",
    "            in_params = True\n",
    "            current_param = None\n",
    "        elif stripped.startswith('Returns:') or stripped.startswith('Raises:'):\n",
    "            in_params = False\n",
    "        elif in_params:\n",
    "            # Parse parameter line (handles \"param: desc\" and \"- param: desc\" formats)\n",
    "            if ':' in stripped and (stripped[0].isalpha() or stripped.startswith(('-', '*'))):\n",
    "                param_name = stripped.lstrip('- *').split(':')[0].strip()\n",
    "                param_desc = ':'.join(stripped.lstrip('- *').split(':')[1:]).strip()\n",
    "                params[param_name] = param_desc\n",
    "                current_param = param_name\n",
    "            elif current_param and stripped:\n",
    "                # Continuation of previous parameter description\n",
    "                params[current_param] += ' ' + stripped\n",
    "    \n",
    "    return params\n",
    "\n",
    "\n",
    "def get_tool_descriptions_from_node(tool_node):\n",
    "    \"\"\"Extract tool descriptions from the ToolNode object.\"\"\"\n",
    "    descriptions = []\n",
    "    \n",
    "    if hasattr(tool_node, 'tools_by_name'):\n",
    "        tools_by_name = tool_node.tools_by_name\n",
    "        \n",
    "        for tool_name, tool in tools_by_name.items():\n",
    "            function_string = inspect.getsource(globals()[tool_name])\n",
    "            # function_string = inspect.getsource(getattr(tool_name))\n",
    "            result = parse_function_definition(function_string)\n",
    "\n",
    "            if result:\n",
    "                descriptions.append(result)\n",
    "    \n",
    "    return descriptions if descriptions else \"Could not extract tool descriptions\"\n",
    "\n",
    "\n",
    "def lc_messages_to_regular_messages(msg):\n",
    "\n",
    "    if isinstance(msg, dict):\n",
    "        \n",
    "        if msg.get(\"role\") == \"user\":\n",
    "            return {\"role\": \"user\", \"content\": msg[\"content\"]}\n",
    "        elif msg.get(\"role\") == \"assistant\":\n",
    "            return {\"role\": \"assistant\", \"content\": msg[\"content\"]}\n",
    "        elif msg.get(\"role\") == \"tool\":\n",
    "            return {\n",
    "                \"role\": \"tool\", \n",
    "                \"content\": msg[\"content\"], \n",
    "                \"tool_call_id\": msg.get(\"tool_call_id\")\n",
    "            }\n",
    "        \n",
    "    elif isinstance(msg, AIMessage):\n",
    "\n",
    "        result = {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": msg.content\n",
    "        }\n",
    "        \n",
    "        if hasattr(msg, 'tool_calls') and msg.tool_calls and len(msg.tool_calls) > 0 and not msg.tool_calls[0].get(\"name\").startswith(\"functions.\"):\n",
    "            result[\"tool_calls\"] = [\n",
    "                {\n",
    "                    \"id\": tc[\"id\"],\n",
    "                    \"type\": \"function\",\n",
    "                    \"function\": {\n",
    "                        \"name\": tc[\"name\"].replace(\"functions.\", \"\"),\n",
    "                        \"arguments\": json.dumps(tc[\"args\"])\n",
    "                    }\n",
    "                }\n",
    "                for tc in msg.tool_calls\n",
    "            ]\n",
    "            \n",
    "        return result\n",
    "    \n",
    "    elif isinstance(msg, ToolMessage):\n",
    "\n",
    "        return {\"role\": \"tool\", \"content\": msg.content, \"tool_call_id\": msg.tool_call_id}\n",
    "    \n",
    "    else:\n",
    "\n",
    "        return {\"role\": \"user\", \"content\": str(msg)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
