{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "import instructor\n",
    "from litellm import completion\n",
    "\n",
    "from openai import OpenAI\n",
    "from langchain_core.messages import AIMessage, ToolMessage\n",
    "\n",
    "import json\n",
    "\n",
    "from typing import List, Dict, Any, Annotated, Optional\n",
    "from operator import add\n",
    "from jinja2 import Template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Coordinator Agent with LiteLLM Router and Fallback model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Delegation(BaseModel):\n",
    "    agent: str\n",
    "    task: str = Field(default=\"\")\n",
    "\n",
    "\n",
    "class CoordinatorAgentResponse(BaseModel):\n",
    "    next_agent: str\n",
    "    plan: list[Delegation]\n",
    "    final_answer: bool = Field(default=False)\n",
    "    answer: str\n",
    "\n",
    "\n",
    "class MCPToolCall(BaseModel):\n",
    "    name: str\n",
    "    arguments: dict\n",
    "    server: str\n",
    "\n",
    "class ToolCall(BaseModel):\n",
    "    name: str\n",
    "    arguments: dict\n",
    "\n",
    "\n",
    "class RAGUsedContext(BaseModel):\n",
    "    id: str\n",
    "    description: str\n",
    "\n",
    "\n",
    "class State(BaseModel):\n",
    "    messages: Annotated[List[Any], add] = []\n",
    "    answer: str = \"\"\n",
    "    product_qa_iteration: int = Field(default=0)\n",
    "    shopping_cart_iteration: int = Field(default=0)\n",
    "    coordinator_iteration: int = Field(default=0)\n",
    "    product_qa_final_answer: bool = Field(default=False)\n",
    "    shopping_cart_final_answer: bool = Field(default=False)\n",
    "    coordinator_final_answer: bool = Field(default=False)\n",
    "    product_qa_available_tools: List[Dict[str, Any]] = []\n",
    "    shopping_cart_available_tools: List[Dict[str, Any]] = []\n",
    "    mcp_tool_calls: Optional[List[MCPToolCall]] = Field(default_factory=list)\n",
    "    tool_calls: Optional[List[ToolCall]] = Field(default_factory=list)\n",
    "    retrieved_context_ids: List[RAGUsedContext] = []\n",
    "    trace_id: str = \"\"\n",
    "    user_id: str = \"\"\n",
    "    cart_id: str = \"\"\n",
    "    plan: list[Delegation] = Field(default_factory=list)\n",
    "    next_agent: str = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lc_messages_to_regular_messages(msg):\n",
    "\n",
    "    if isinstance(msg, dict):\n",
    "        \n",
    "        if msg.get(\"role\") == \"user\":\n",
    "            return {\"role\": \"user\", \"content\": msg[\"content\"]}\n",
    "        elif msg.get(\"role\") == \"assistant\":\n",
    "            return {\"role\": \"assistant\", \"content\": msg[\"content\"]}\n",
    "        elif msg.get(\"role\") == \"tool\":\n",
    "            return {\n",
    "                \"role\": \"tool\", \n",
    "                \"content\": msg[\"content\"], \n",
    "                \"tool_call_id\": msg.get(\"tool_call_id\")\n",
    "            }\n",
    "        \n",
    "    elif isinstance(msg, AIMessage):\n",
    "\n",
    "        result = {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": msg.content\n",
    "        }\n",
    "        \n",
    "        if hasattr(msg, 'tool_calls') and msg.tool_calls and len(msg.tool_calls) > 0 and not msg.tool_calls[0].get(\"name\").startswith(\"functions.\"):\n",
    "            result[\"tool_calls\"] = [\n",
    "                {\n",
    "                    \"id\": tc[\"id\"],\n",
    "                    \"type\": \"function\",\n",
    "                    \"function\": {\n",
    "                        \"name\": tc[\"name\"].replace(\"functions.\", \"\"),\n",
    "                        \"arguments\": json.dumps(tc[\"args\"])\n",
    "                    }\n",
    "                }\n",
    "                for tc in msg.tool_calls\n",
    "            ]\n",
    "            \n",
    "        return result\n",
    "    \n",
    "    elif isinstance(msg, ToolMessage):\n",
    "\n",
    "        return {\"role\": \"tool\", \"content\": msg.content, \"tool_call_id\": msg.tool_call_id}\n",
    "    \n",
    "    else:\n",
    "\n",
    "        return {\"role\": \"user\", \"content\": str(msg)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coordinator_agent_node(state, models = [\"gpt-4.1\", \"groq/llama-3.3-70b-versatile\"]) -> dict:\n",
    "\n",
    "    prompt_template = \"\"\"You are a Coordinator Agent as part of a shopping assistant.\n",
    "\n",
    "Your role is to create plans for solving user queries and delegate the tasks accordingly.\n",
    "You will be given a conversation history, your task is to create a plan for solving the user's query.\n",
    "After the plan is created, you should output the next agent to invoke and the task to be performed by that agent.\n",
    "Once an agent finishes its task, you will be handed the control back, you should then review the conversation history and revise the plan.\n",
    "If there is a sequence of tasks to be performed by a single agent, you should combine them into a single task.\n",
    "\n",
    "The possible agents are:\n",
    "\n",
    "- product_qa_agent: The user is asking a question about a product. This can be a question about available products, their specifications, user reviews etc.\n",
    "- shopping_cart_agent: The user is asking to add or remove items from the shopping cart or questions about the current shopping cart.\n",
    "\n",
    "CRITICAL RULES:\n",
    "- If next_agent is \"\", final_answer MUST be false\n",
    "(You cannot delegate the task to an agent and return to the user in the same response)\n",
    "- If final_answer is true, next_agent MUST be \"\"\n",
    "(You must wait for agent results before returning to user)\n",
    "- If you need to call other agents before answering, set:\n",
    "next_agent=\"...\", final_answer=false\n",
    "- After receiving agent results, you can then set:\n",
    "next_agent=\"\", final_answer=true\n",
    "\n",
    "Additional instructions:\n",
    "\n",
    "- Write the plan to the plan field.\n",
    "- Write the next agent to invoke to the next_agent field.\n",
    "- Once you have all the information needed to answer the user's query, you should set the final_answer field to True and output the answer to the user's query.\n",
    "- The final answer to the user query should be a comprehensive answer that explains the actions that were performed to answer the query.\n",
    "- Never set final_answer to true if the plan is not complete.\n",
    "- You should output the next_agent field as well as the plan field.\n",
    "\"\"\"\n",
    "\n",
    "    prompt = Template(prompt_template).render()\n",
    "\n",
    "    messages = state.messages\n",
    "\n",
    "    conversation = []\n",
    "\n",
    "    for msg in messages:\n",
    "        conversation.append(lc_messages_to_regular_messages(msg))\n",
    "\n",
    "    client = instructor.from_litellm(completion)\n",
    "\n",
    "    for model in models:\n",
    "        try:\n",
    "            response, raw_response = client.chat.completions.create_with_completion(\n",
    "                model=model,\n",
    "                response_model=CoordinatorAgentResponse,\n",
    "                messages=[{\"role\": \"system\", \"content\": prompt}, *conversation],\n",
    "                temperature=0,\n",
    "            )\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Error with model {model}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if response.final_answer:\n",
    "        ai_message = [AIMessage(\n",
    "            content=response.answer,\n",
    "        )]\n",
    "    else:\n",
    "        ai_message = []\n",
    "\n",
    "    return {\n",
    "        \"messages\": ai_message,\n",
    "        \"answer\": response.answer,\n",
    "        \"next_agent\": response.next_agent,\n",
    "        \"plan\": response.plan,\n",
    "        \"coordinator_final_answer\": response.final_answer,\n",
    "        \"coordinator_iteration\": state.coordinator_iteration + 1,\n",
    "        \"trace_id\": \"\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state = State(messages=[{\"role\": \"user\", \"content\": \"What is the weather today?\"}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = coordinator_agent_node(initial_state, models=[\"groq/llama-3.3-70b-versatile\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
