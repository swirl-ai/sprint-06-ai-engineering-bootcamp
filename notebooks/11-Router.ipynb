{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57fdc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastmcp import Client\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Prefetch, Filter, FieldCondition, MatchText, FusionQuery\n",
    "\n",
    "from langsmith import traceable, get_current_run_tree\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "from langchain_core.messages import AIMessage, ToolMessage\n",
    "\n",
    "from jinja2 import Template\n",
    "from typing import Literal, Dict, Any, Annotated, List, Optional\n",
    "from IPython.display import Image, display\n",
    "from operator import add\n",
    "from openai import OpenAI\n",
    "\n",
    "import openai\n",
    "\n",
    "import random\n",
    "import ast\n",
    "import inspect\n",
    "import instructor\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb45461",
   "metadata": {},
   "source": [
    "### List available tools in MCP servers running on http://localhost:8001/mcp and http://localhost:8002/mcp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9b8acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(\"http://localhost:8001/mcp\")\n",
    "\n",
    "async with client:\n",
    "    # List available resources\n",
    "    tools = await client.list_tools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8526333",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf2bb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tools[0].name)\n",
    "print(tools[0].description)\n",
    "print(tools[0].inputSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6511d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(\"http://localhost:8002/mcp\")\n",
    "\n",
    "async with client:\n",
    "    # List available resources\n",
    "    tools = await client.list_tools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d7b7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tools[0].name)\n",
    "print(tools[0].description)\n",
    "print(tools[0].inputSchema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53905264",
   "metadata": {},
   "source": [
    "### Execute a tool on one of the running MCP Servers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3943bc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(\"http://localhost:8001/mcp\")\n",
    "\n",
    "async with client:\n",
    "    # List available resources\n",
    "    result = await client.call_tool(\"get_formatted_item_context\", {\"query\": \"earphones\", \"top_k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1ab670",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee90198",
   "metadata": {},
   "source": [
    "### A function to extract tool definitions of all available tools in provided MCP servers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c38759b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_docstring_params(docstring: str) -> Dict[str, str]:\n",
    "    \"\"\"Extract parameter descriptions from docstring (handles both Args: and Parameters: formats).\"\"\"\n",
    "    params = {}\n",
    "    lines = docstring.split('\\n')\n",
    "    in_params = False\n",
    "    current_param = None\n",
    "    \n",
    "    for line in lines:\n",
    "        stripped = line.strip()\n",
    "        \n",
    "        # Check for parameter section start\n",
    "        if stripped in ['Args:', 'Arguments:', 'Parameters:', 'Params:']:\n",
    "            in_params = True\n",
    "            current_param = None\n",
    "        elif stripped.startswith('Returns:') or stripped.startswith('Raises:'):\n",
    "            in_params = False\n",
    "        elif in_params:\n",
    "            # Parse parameter line (handles \"param: desc\" and \"- param: desc\" formats)\n",
    "            if ':' in stripped and (stripped[0].isalpha() or stripped.startswith(('-', '*'))):\n",
    "                param_name = stripped.lstrip('- *').split(':')[0].strip()\n",
    "                param_desc = ':'.join(stripped.lstrip('- *').split(':')[1:]).strip()\n",
    "                params[param_name] = param_desc\n",
    "                current_param = param_name\n",
    "            elif current_param and stripped:\n",
    "                # Continuation of previous parameter description\n",
    "                params[current_param] += ' ' + stripped\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78593a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_tool_descriptions_from_mcp_servers(mcp_servers: list[str]) -> list[dict]:\n",
    "\n",
    "    tool_descriptions = []\n",
    "\n",
    "    for server in mcp_servers:\n",
    "\n",
    "        client = Client(server)\n",
    "\n",
    "        async with client:\n",
    "\n",
    "            tools = await client.list_tools()\n",
    "\n",
    "            for tool in tools:\n",
    "                \n",
    "                result = {\n",
    "                    \"name\": \"\",\n",
    "                    \"description\": \"\",\n",
    "                    \"parameters\": {\"type\": \"object\", \"properties\": {}},\n",
    "                    \"required\": [],\n",
    "                    \"returns\": {\"type\": \"string\", \"description\": \"\"},\n",
    "                    \"server\": server\n",
    "                }\n",
    "\n",
    "                result[\"name\"] = tool.name\n",
    "                result[\"required\"] = tool.inputSchema.get(\"required\", [])\n",
    "\n",
    "                ## Get Description\n",
    "\n",
    "                description = tool.description.split(\"\\n\\n\")[0]\n",
    "                result[\"description\"] = description\n",
    "\n",
    "\n",
    "                ## Get Returns\n",
    "\n",
    "                returns = tool.description.split(\"Returns:\")[1].strip()\n",
    "                result[\"returns\"][\"description\"] = returns\n",
    "\n",
    "                ## Get parameters\n",
    "\n",
    "                property_descriptions = parse_docstring_params(tool.description)\n",
    "                properties = tool.inputSchema.get(\"properties\", {})\n",
    "                for key, value in properties.items():\n",
    "                    properties[key][\"description\"] = property_descriptions.get(key, \"\")\n",
    "\n",
    "                result[\"parameters\"][\"properties\"] = properties\n",
    "\n",
    "                tool_descriptions.append(result)\n",
    "\n",
    "    return tool_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b70d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcp_servers = [\"http://localhost:8001/mcp\", \"http://localhost:8002/mcp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e6879e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_descriptions = await get_tool_descriptions_from_mcp_servers(mcp_servers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c781b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2beecc8",
   "metadata": {},
   "source": [
    "## Agent integration with tools exposed via MCP Servers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990d9791",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad134143",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lc_messages_to_regular_messages(msg):\n",
    "\n",
    "    if isinstance(msg, dict):\n",
    "        \n",
    "        if msg.get(\"role\") == \"user\":\n",
    "            return {\"role\": \"user\", \"content\": msg[\"content\"]}\n",
    "        elif msg.get(\"role\") == \"assistant\":\n",
    "            return {\"role\": \"assistant\", \"content\": msg[\"content\"]}\n",
    "        elif msg.get(\"role\") == \"tool\":\n",
    "            return {\n",
    "                \"role\": \"tool\", \n",
    "                \"content\": msg[\"content\"], \n",
    "                \"tool_call_id\": msg.get(\"tool_call_id\")\n",
    "            }\n",
    "        \n",
    "    elif isinstance(msg, AIMessage):\n",
    "\n",
    "        result = {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": msg.content\n",
    "        }\n",
    "        \n",
    "        if hasattr(msg, 'tool_calls') and msg.tool_calls and len(msg.tool_calls) > 0 and not msg.tool_calls[0].get(\"name\").startswith(\"functions.\"):\n",
    "            result[\"tool_calls\"] = [\n",
    "                {\n",
    "                    \"id\": tc[\"id\"],\n",
    "                    \"type\": \"function\",\n",
    "                    \"function\": {\n",
    "                        \"name\": tc[\"name\"].replace(\"functions.\", \"\"),\n",
    "                        \"arguments\": json.dumps(tc[\"args\"])\n",
    "                    }\n",
    "                }\n",
    "                for tc in msg.tool_calls\n",
    "            ]\n",
    "            \n",
    "        return result\n",
    "    \n",
    "    elif isinstance(msg, ToolMessage):\n",
    "\n",
    "        return {\"role\": \"tool\", \"content\": msg.content, \"tool_call_id\": msg.tool_call_id}\n",
    "    \n",
    "    else:\n",
    "\n",
    "        return {\"role\": \"user\", \"content\": str(msg)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075422ec",
   "metadata": {},
   "source": [
    "### Pydantic models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b76c05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToolCall(BaseModel):\n",
    "    name: str\n",
    "    arguments: dict\n",
    "    server: str\n",
    "\n",
    "class RAGUsedContext(BaseModel):\n",
    "    id: int\n",
    "    description: str\n",
    "\n",
    "class ProductQAAgentResponse(BaseModel):\n",
    "    answer: str\n",
    "    tool_calls: List[ToolCall] = Field(default_factory=list)\n",
    "    final_answer: bool = Field(default=False)\n",
    "    retrieved_context_ids: List[RAGUsedContext]\n",
    "\n",
    "class State(BaseModel):\n",
    "    messages: Annotated[List[Any], add] = []\n",
    "    answer: str = \"\"\n",
    "    iteration: int = Field(default=0)\n",
    "    final_answer: bool = Field(default=False)\n",
    "    available_tools: List[Dict[str, Any]] = []\n",
    "    tool_calls: Optional[List[ToolCall]] = Field(default_factory=list)\n",
    "    retrieved_context_ids: List[RAGUsedContext] = Field(default_factory=list)\n",
    "    user_intent: str = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116a386b",
   "metadata": {},
   "source": [
    "### The Product QA Agent Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a994738b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@traceable(\n",
    "    name=\"product_qa_agent\",\n",
    "    run_type=\"llm\",\n",
    "    metadata={\"ls_provider\": \"openai\", \"ls_model_name\": \"gpt-4.1\"}\n",
    ")\n",
    "def product_qa_agent_node(state: State) -> dict:\n",
    "\n",
    "   prompt_template =  \"\"\"You are a shopping assistant that can answer questions about the products in stock.\n",
    "\n",
    "You will be given a question and a list of tools you can use to answer that question.\n",
    "\n",
    "<Available tools>\n",
    "{{ available_tools | tojson }}\n",
    "</Available tools>\n",
    "\n",
    "After the tools are used you will get the outputs from the tools.\n",
    "\n",
    "When you need to use a tool, format your response as:\n",
    "\n",
    "<tool_call>\n",
    "{\"name\": \"tool_name\", \"arguments\": {...}}\n",
    "</tool_call>\n",
    "\n",
    "Use names specificly provided in the available tools. Don't add any additional text to the names.\n",
    "\n",
    "You should tend to use tools when additional information is needed to answer the question.\n",
    "\n",
    "If you set final_answer to True, you should not use any tools.\n",
    "\n",
    "Instructions:\n",
    "- You need to answer the question based on the retrieved context using the available tools only.\n",
    "- Never use word context and refer to it as the available products.\n",
    "- You should only answer questions about the products in stock. If the question is not about the products in stock, you should ask for clarification.\n",
    "- As a final output you need to provide:\n",
    "\n",
    "* The answer to the question based on the retrieved context.\n",
    "* The list of the indexes from the chunks returned from all tool calls that were used to answer the question. If more than one chunk was used to compile the answer from a single tool call, be sure to return all of them.\n",
    "* Short description of the item based on the retrieved context.\n",
    "\n",
    "- The answer to the question should contain detailed information about the product and returned with detailed specification in bullet points.\n",
    "- The short description should have the name of the item.\n",
    "- If the user's request requires using a tool, set tool_calls with the appropriate function name and arguments.\n",
    "- If you have all the information needed to provide a complete answer, set final_answer to True.\n",
    "\"\"\"\n",
    "\n",
    "   template = Template(prompt_template)\n",
    "   \n",
    "   prompt = template.render(\n",
    "      available_tools=state.available_tools\n",
    "   )\n",
    "\n",
    "   messages = state.messages\n",
    "\n",
    "   conversation = []\n",
    "\n",
    "   for msg in messages:\n",
    "      conversation.append(lc_messages_to_regular_messages(msg))\n",
    "\n",
    "   client = instructor.from_openai(OpenAI())\n",
    "\n",
    "   response, raw_response = client.chat.completions.create_with_completion(\n",
    "        model=\"gpt-4.1\",\n",
    "        response_model=ProductQAAgentResponse,\n",
    "        messages=[{\"role\": \"system\", \"content\": prompt}, *conversation],\n",
    "        temperature=0,\n",
    "   )\n",
    "\n",
    "   if response.tool_calls:\n",
    "      tool_calls = []\n",
    "      for i, tc in enumerate(response.tool_calls):\n",
    "         tool_calls.append({\n",
    "               \"id\": f\"call_{i}\",\n",
    "               \"name\": tc.name,\n",
    "               \"args\": tc.arguments\n",
    "         })\n",
    "\n",
    "      ai_message = AIMessage(\n",
    "         content=response.answer,\n",
    "         tool_calls=tool_calls\n",
    "         )\n",
    "   else:\n",
    "      ai_message = AIMessage(\n",
    "         content=response.answer,\n",
    "      )\n",
    "\n",
    "   return {\n",
    "      \"messages\": [ai_message],\n",
    "      \"tool_calls\": response.tool_calls,\n",
    "      \"iteration\": state.iteration + 1,\n",
    "      \"answer\": response.answer,\n",
    "      \"final_answer\": response.final_answer,\n",
    "      \"retrieved_context_ids\": response.retrieved_context_ids\n",
    "   }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be362ed",
   "metadata": {},
   "source": [
    "### User Intent Router node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9bbb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntentRouterAgentResponse(BaseModel):\n",
    "    user_intent: str\n",
    "    answer: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61982b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@traceable(\n",
    "    name=\"intent_router_agent\",\n",
    "    run_type=\"llm\",\n",
    "    metadata={\"ls_provider\": \"openai\", \"ls_model_name\": \"gpt-4.1\"}\n",
    ")\n",
    "def intent_router_agent_node(state: State) -> dict:\n",
    "\n",
    "   prompt_template =  \"\"\"You are a part of a shopping assistant that routes user queries to the appropriate agents.\n",
    "\n",
    "You will be given a conversation history, your task is to classify the intent of the user's latest query and output an appropriate classification.\n",
    "\n",
    "The possible intents are:\n",
    "\n",
    "- product_qa: The user is asking a question about a product. This can be a question about available products, their specifications, user reviews etc.\n",
    "- other: The user's latest query is not clear or not related to the shopping assistant.\n",
    "\n",
    "Additional instructions:\n",
    "\n",
    "- Write the intent classification to the user_intent field.\n",
    "- If the classification is 'other', you should output the answer to the user's query trying to clarify the user's intent.\n",
    "- If the classification is 'product_qa', you should only output the intent classification and no other text.\n",
    "\"\"\"\n",
    "\n",
    "   template = Template(prompt_template)\n",
    "   \n",
    "   prompt = template.render()\n",
    "\n",
    "   messages = state.messages\n",
    "\n",
    "   conversation = []\n",
    "\n",
    "   for msg in messages:\n",
    "      conversation.append(lc_messages_to_regular_messages(msg))\n",
    "\n",
    "   client = instructor.from_openai(OpenAI())\n",
    "\n",
    "   response, raw_response = client.chat.completions.create_with_completion(\n",
    "        model=\"gpt-4.1\",\n",
    "        response_model=IntentRouterAgentResponse,\n",
    "        messages=[{\"role\": \"system\", \"content\": prompt}, *conversation],\n",
    "        temperature=0,\n",
    "   )\n",
    "\n",
    "   if response.user_intent == \"product_qa\":\n",
    "      ai_message = []\n",
    "   else:\n",
    "      ai_message = [AIMessage(\n",
    "         content=response.answer,\n",
    "      )]\n",
    "\n",
    "   return {\n",
    "      \"messages\": ai_message,\n",
    "      \"answer\": response.answer,\n",
    "      \"user_intent\": response.user_intent\n",
    "   }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3441856",
   "metadata": {},
   "source": [
    "### The Tool Use Router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce06547d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tool_router(state: State) -> str:\n",
    "    \"\"\"Decide whether to continue or end\"\"\"\n",
    "    \n",
    "    if state.final_answer:\n",
    "        return \"end\"\n",
    "    elif state.iteration > 2:\n",
    "        return \"end\"\n",
    "    elif len(state.tool_calls) > 0:\n",
    "        return \"tools\"\n",
    "    else:\n",
    "        return \"end\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3fd74b",
   "metadata": {},
   "source": [
    "### User Intent Router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fb1bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_intent_router(state) -> str:\n",
    "    \"\"\"Decide whether to continue or end\"\"\"\n",
    "    \n",
    "    if state.user_intent == \"product_qa\":\n",
    "        return \"product_qa_agent\"\n",
    "    else:\n",
    "        return \"end\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5025df65",
   "metadata": {},
   "source": [
    "### Custom tool node function that supports running tools exposed via MCP Servers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b60b44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def mcp_tool_node(state: State) -> str:\n",
    "\n",
    "    tool_messages = []\n",
    "\n",
    "    for i, tc in enumerate(state.tool_calls):\n",
    "\n",
    "        client = Client(tc.server)\n",
    "\n",
    "        async with client:\n",
    "\n",
    "            result = await client.call_tool(tc.name, tc.arguments)\n",
    "\n",
    "            tool_message = ToolMessage(\n",
    "                content=result,\n",
    "                tool_call_id=f\"call_{i}\"\n",
    "            )\n",
    "\n",
    "            tool_messages.append(tool_message)\n",
    "\n",
    "    return {\n",
    "        \"messages\": tool_messages\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4f5eb3",
   "metadata": {},
   "source": [
    "### LangGraph Grapg implementation with MCP support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccb5c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(State)\n",
    "\n",
    "mcp_servers = [\"http://localhost:8001/mcp\", \"http://localhost:8002/mcp\"]\n",
    "\n",
    "tool_descriptions = await get_tool_descriptions_from_mcp_servers(mcp_servers)\n",
    "\n",
    "workflow.add_edge(START, \"intent_router_agent_node\")\n",
    "\n",
    "workflow.add_node(\"intent_router_agent_node\", intent_router_agent_node)\n",
    "workflow.add_node(\"product_qa_agent_node\", product_qa_agent_node)\n",
    "workflow.add_node(\"mcp_tool_node\", mcp_tool_node)\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"intent_router_agent_node\",\n",
    "    user_intent_router,\n",
    "    {\n",
    "        \"product_qa_agent\": \"product_qa_agent_node\",\n",
    "        \"end\": END\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"product_qa_agent_node\",\n",
    "    tool_router,\n",
    "    {\n",
    "        \"tools\": \"mcp_tool_node\",\n",
    "        \"end\": END\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"mcp_tool_node\", \"product_qa_agent_node\")\n",
    "\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16a0566",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f9594e",
   "metadata": {},
   "source": [
    "### Invoke the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c6c20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state = {\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"What is the weather today?\"}],\n",
    "    \"available_tools\": tool_descriptions\n",
    "}\n",
    "result = await graph.ainvoke(initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd458ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state = {\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Can I get earphones for myself, a laptop bag for my wife and something cool for my kids?\"}],\n",
    "    \"available_tools\": tool_descriptions\n",
    "}\n",
    "result = await graph.ainvoke(initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98336862",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0854bf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
